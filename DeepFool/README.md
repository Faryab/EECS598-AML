# DeepFool presentation

#### Songlin Liu, Zihan Wang, Bingzhao Shan

#### Feb 09, 2021


## Paper
DeepFool Original Paper:   [HERE](https://arxiv.org/pdf/1511.04599.pdf)

FGSM Original Paper: [HERE](https://arxiv.org/pdf/1412.6572.pdf)

## Code reading guidance

We did the replication experiment on a two-layer fully connected neural network on the MNIST dataset. Our experiments include:

- **Implemented the experimental neural network, the robustness function and the visual demonstration codes.** [Section2, Section3, Section 4]

- **Generated adversarial samples using DeepFool as the experimental group and FGSM as the control group. Adversarial samples on the MNIST dataset are shown in the colab file.** [Section 3]

- **Attacked the neural network using DeepFool and FGSM correspondingly and did quantitative analysis on the results (accuracy and speed).** [Section 4]

- **Fine-tuned the model using adversarial samples generated by DeepFool and FGSM. Evaluated the fine-tuned models.** [Section 5]

- **Reproduced the over-perturbed experiment in the fine-tuning process.** [Section 5]


## Replication summary [consistency v.s. inconsistency]

- Most results are consistent with the original paper.

- In section3, we showed that under the same attacking result, the perturbation generated by DeepFool is small compared to FGSM. This is **consistent** with the paper.

- In section4, we showed that DeepFool is a stronger attack compared to FGSM. In other words, the misclassification rate is higher when using DeepFool. This is also **consistent** with the paper.

- In section5, the fine-tuning result is also consistent with the paper. One small **inconsistency** is that the figure we get in section 5.1, though DeepFool is still better than FGSM, FGSM does not harm the robustness of the original model, which is slightly different from the figure illustrated in the original paper.

- In section5, the figure generated using over-perturbed data is also **consistent** with the Paper.

- [Additional experiments] Other than these replications, we also showed that fine-tuning the model using proper adversarial samples won't harm the accuracy of the original dataset too much (sometimes we even got improvement) [Figure 5.2]. However, fine-tuning the dataset using over-perturbed data can harm the test accuracy on the original dataset [Figure 5.4].
